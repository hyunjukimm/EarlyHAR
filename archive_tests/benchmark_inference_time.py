#!/usr/bin/env python3
"""
ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

ë…¼ë¦¬ì  earlinessì™€ ì‹¤ì œ ì‹œìŠ¤í…œ ì‹œê°„ì„ í•¨ê»˜ ì¸¡ì •í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤.

Usage:
    python benchmark_inference_time.py --dataset doore --fold 0
"""

import argparse
import time
import torch
import pickle
import numpy as np
from pathlib import Path

from baselines.calimera import CALIMERA
from baselines.EARLIEST.model import EARLIEST
from baselines.StopAndHop.model import StopAndHop
from data_preprocessing.data_preprocess import pad_sequences

def setup_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type='str', default='doore')
    parser.add_argument('--fold', type='int', default=0)
    parser.add_argument('--num_samples', type='int', default=100, help='Number of samples to benchmark')
    parser.add_argument('--warmup', type='int', default=5, help='Warmup iterations')
    parser.add_argument('--device', type='str', default='cuda' if torch.cuda.is_available() else 'cpu')
    return parser.parse_args()

def load_test_data(dataset, fold_idx):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    fold_path = f'fold_data/{dataset}/fold_{fold_idx}.pkl'
    with open(fold_path, 'rb') as f:
        fold_data = pickle.load(f)
    
    test_data = fold_data['test_data']
    test_labels = fold_data['test_labels']
    
    return test_data, test_labels

def benchmark_calimera(test_data, test_labels, num_samples, warmup):
    """CALIMERA ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
    print("\n" + "="*80)
    print("CALIMERA Benchmark")
    print("="*80)
    
    # ë°ì´í„° ì¤€ë¹„
    test_tensor, _ = pad_sequences(test_data[:num_samples], padding_type='mean')
    X_test = test_tensor.permute(0, 2, 1).numpy()
    y_test = test_labels[:num_samples].numpy() if torch.is_tensor(test_labels) else np.array(test_labels[:num_samples])
    
    # ëª¨ë¸ í•™ìŠµ (í•„ìš”í•œ ê²½ìš° - ì‹¤ì œë¡œëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)
    print("Training CALIMERA (for benchmark)...")
    model = CALIMERA(delay_penalty=1)
    
    # ê°„ë‹¨í•œ í•™ìŠµ ë°ì´í„°ë¡œ fit (ì‹¤ì œë¡œëŠ” ì „ì²´ ë°ì´í„° ì‚¬ìš©)
    train_subset = min(50, len(test_data))
    train_tensor, _ = pad_sequences(test_data[:train_subset], padding_type='mean')
    X_train = train_tensor.permute(0, 2, 1).numpy()
    y_train = test_labels[:train_subset].numpy() if torch.is_tensor(test_labels) else np.array(test_labels[:train_subset])
    
    fit_start = time.time()
    model.fit(X_train, y_train)
    fit_time = time.time() - fit_start
    print(f"Training time: {fit_time:.2f}s")
    
    # Warmup
    print(f"\nWarmup ({warmup} iterations)...")
    for _ in range(warmup):
        _ = model.test(X_test[:5])
    
    # ì‹¤ì œ ì¸¡ì •
    print(f"\nBenchmarking on {num_samples} samples...")
    inference_times = []
    
    for i in range(num_samples):
        X_single = X_test[i:i+1]
        
        start = time.perf_counter()
        stop_timestamps, y_pred = model.test(X_single)
        end = time.perf_counter()
        
        inference_times.append((end - start) * 1000)  # ms
    
    # í†µê³„
    mean_time = np.mean(inference_times)
    std_time = np.std(inference_times)
    median_time = np.median(inference_times)
    min_time = np.min(inference_times)
    max_time = np.max(inference_times)
    
    print(f"\n{'Metric':<20} {'Value':>15}")
    print("-" * 40)
    print(f"{'Mean inference time':<20} {mean_time:>12.2f} ms")
    print(f"{'Std deviation':<20} {std_time:>12.2f} ms")
    print(f"{'Median':<20} {median_time:>12.2f} ms")
    print(f"{'Min':<20} {min_time:>12.2f} ms")
    print(f"{'Max':<20} {max_time:>12.2f} ms")
    print(f"{'Throughput':<20} {1000/mean_time:>12.2f} samples/s")
    
    return {
        'mean': mean_time,
        'std': std_time,
        'median': median_time,
        'min': min_time,
        'max': max_time,
        'throughput': 1000/mean_time
    }

def benchmark_earliest(test_data, test_labels, num_samples, warmup, device):
    """EARLIEST ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
    print("\n" + "="*80)
    print("EARLIEST Benchmark")
    print("="*80)
    
    # ë°ì´í„° ì¤€ë¹„
    test_tensor, _ = pad_sequences(test_data[:num_samples], padding_type='mean')
    test_tensor = test_tensor.to(device)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    input_channels = test_tensor.shape[2]
    num_classes = len(torch.unique(test_labels[:num_samples]))
    
    model = EARLIEST(
        ninp=input_channels,
        nhid=64,
        rnn_cell='LSTM',
        nlayers=1,
        nclasses=num_classes,
        lam=0.0
    ).to(device)
    model.eval()
    
    # Warmup
    print(f"\nWarmup ({warmup} iterations)...")
    with torch.no_grad():
        for _ in range(warmup):
            X_warmup = test_tensor[:5].permute(1, 0, 2)  # (T, B, V)
            _ = model(X_warmup, test=True)
    
    # ì‹¤ì œ ì¸¡ì •
    print(f"\nBenchmarking on {num_samples} samples...")
    inference_times = []
    halt_points_list = []
    
    with torch.no_grad():
        for i in range(num_samples):
            X_single = test_tensor[i:i+1].permute(1, 0, 2)  # (T, 1, V)
            
            start = time.perf_counter()
            logits, _, halt_points = model(X_single, test=True)
            end = time.perf_counter()
            
            inference_times.append((end - start) * 1000)  # ms
            halt_points_list.append(halt_points.cpu().numpy())
    
    # í†µê³„
    mean_time = np.mean(inference_times)
    std_time = np.std(inference_times)
    median_time = np.median(inference_times)
    min_time = np.min(inference_times)
    max_time = np.max(inference_times)
    
    # Earliness ê³„ì‚°
    max_T = test_tensor.shape[1]
    avg_halt = np.mean([np.mean(hp) for hp in halt_points_list])
    earliness = avg_halt / max_T
    
    print(f"\n{'Metric':<20} {'Value':>15}")
    print("-" * 40)
    print(f"{'Mean inference time':<20} {mean_time:>12.2f} ms")
    print(f"{'Std deviation':<20} {std_time:>12.2f} ms")
    print(f"{'Median':<20} {median_time:>12.2f} ms")
    print(f"{'Min':<20} {min_time:>12.2f} ms")
    print(f"{'Max':<20} {max_time:>12.2f} ms")
    print(f"{'Throughput':<20} {1000/mean_time:>12.2f} samples/s")
    print(f"{'Avg halt point':<20} {avg_halt:>12.2f}")
    print(f"{'Earliness':<20} {earliness*100:>12.2f} %")
    
    return {
        'mean': mean_time,
        'std': std_time,
        'median': median_time,
        'min': min_time,
        'max': max_time,
        'throughput': 1000/mean_time,
        'earliness': earliness
    }

def benchmark_stopandhop(test_data, test_labels, num_samples, warmup, device):
    """Stop and Hop ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
    print("\n" + "="*80)
    print("Stop and Hop Benchmark")
    print("="*80)
    
    # ë°ì´í„° ì¤€ë¹„
    test_tensor, _ = pad_sequences(test_data[:num_samples], padding_type='mean')
    test_tensor = test_tensor.to(device)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    input_channels = test_tensor.shape[2]
    num_classes = len(torch.unique(test_labels[:num_samples]))
    
    config = {
        'n_epochs': 50,
        'nhid': 64,
        'rnn_cell': 'LSTM',
        'nlayers': 1,
    }
    
    model = StopAndHop(
        ninp=input_channels,
        nclasses=num_classes,
        config=config,
        std=0.1,
        lam=0.0
    ).to(device)
    model.eval()
    
    # Warmup
    print(f"\nWarmup ({warmup} iterations)...")
    with torch.no_grad():
        for _ in range(warmup):
            X_warmup = test_tensor[:5].numpy()
            _ = model(X_warmup, test=True)
    
    # ì‹¤ì œ ì¸¡ì •
    print(f"\nBenchmarking on {num_samples} samples...")
    inference_times = []
    halt_points_list = []
    
    with torch.no_grad():
        for i in range(num_samples):
            X_single = test_tensor[i:i+1].numpy()
            
            start = time.perf_counter()
            logits, _, halt_points = model(X_single, test=True)
            end = time.perf_counter()
            
            inference_times.append((end - start) * 1000)  # ms
            halt_points_list.append(halt_points)
    
    # í†µê³„
    mean_time = np.mean(inference_times)
    std_time = np.std(inference_times)
    median_time = np.median(inference_times)
    min_time = np.min(inference_times)
    max_time = np.max(inference_times)
    
    # Earliness ê³„ì‚°
    max_T = test_tensor.shape[1]
    avg_halt = np.mean(halt_points_list)
    earliness = avg_halt / max_T
    
    print(f"\n{'Metric':<20} {'Value':>15}")
    print("-" * 40)
    print(f"{'Mean inference time':<20} {mean_time:>12.2f} ms")
    print(f"{'Std deviation':<20} {std_time:>12.2f} ms")
    print(f"{'Median':<20} {median_time:>12.2f} ms")
    print(f"{'Min':<20} {min_time:>12.2f} ms")
    print(f"{'Max':<20} {max_time:>12.2f} ms")
    print(f"{'Throughput':<20} {1000/mean_time:>12.2f} samples/s")
    print(f"{'Avg halt point':<20} {avg_halt:>12.2f}")
    print(f"{'Earliness':<20} {earliness*100:>12.2f} %")
    
    return {
        'mean': mean_time,
        'std': std_time,
        'median': median_time,
        'min': min_time,
        'max': max_time,
        'throughput': 1000/mean_time,
        'earliness': earliness
    }

def main():
    args = setup_args()
    
    print("="*80)
    print("ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ë²¤ì¹˜ë§ˆí¬")
    print("="*80)
    print(f"Dataset: {args.dataset}")
    print(f"Fold: {args.fold}")
    print(f"Num samples: {args.num_samples}")
    print(f"Device: {args.device}")
    print(f"Warmup iterations: {args.warmup}")
    
    # ë°ì´í„° ë¡œë“œ
    print("\nLoading test data...")
    test_data, test_labels = load_test_data(args.dataset, args.fold)
    print(f"Loaded {len(test_data)} test samples")
    
    results = {}
    
    # CALIMERA ë²¤ì¹˜ë§ˆí¬
    try:
        results['calimera'] = benchmark_calimera(
            test_data, test_labels, 
            min(args.num_samples, len(test_data)), 
            args.warmup
        )
    except Exception as e:
        print(f"\nâš ï¸  CALIMERA benchmark failed: {e}")
    
    # EARLIEST ë²¤ì¹˜ë§ˆí¬
    try:
        results['earliest'] = benchmark_earliest(
            test_data, test_labels,
            min(args.num_samples, len(test_data)),
            args.warmup,
            args.device
        )
    except Exception as e:
        print(f"\nâš ï¸  EARLIEST benchmark failed: {e}")
    
    # Stop and Hop ë²¤ì¹˜ë§ˆí¬
    try:
        results['stopandhop'] = benchmark_stopandhop(
            test_data, test_labels,
            min(args.num_samples, len(test_data)),
            args.warmup,
            args.device
        )
    except Exception as e:
        print(f"\nâš ï¸  Stop and Hop benchmark failed: {e}")
    
    # ìµœì¢… ë¹„êµ
    print("\n" + "="*80)
    print("ìµœì¢… ë¹„êµ")
    print("="*80)
    
    print(f"\n{'Baseline':<15} {'Mean (ms)':>12} {'Throughput':>15} {'Earliness (%)':>15}")
    print("-" * 80)
    
    for name, result in results.items():
        earliness_str = f"{result['earliness']*100:.2f}" if 'earliness' in result else "N/A"
        print(f"{name:<15} {result['mean']:>12.2f} {result['throughput']:>12.2f} sps {earliness_str:>15}")
    
    print("\nğŸ’¡ í•´ì„:")
    print("  - Mean (ms): í‰ê·  ì¶”ë¡  ì‹œê°„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print("  - Throughput: ì´ˆë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print("  - Earliness: ë…¼ë¦¬ì  ì¤‘ë‹¨ ì‹œì  (ë‚®ì„ìˆ˜ë¡ ë¹ ë¥¸ ì¤‘ë‹¨)")
    print()
    print("âš ï¸  ì£¼ì˜: Earlinessê°€ ë‚®ì•„ë„ ì‹¤ì œ ì¶”ë¡  ì‹œê°„ì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
